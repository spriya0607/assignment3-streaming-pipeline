# ğŸ“¡ IoT Sensor Data Streaming Pipeline (AWS + Spark + Terraform)

This project implements a real-time data streaming pipeline for IoT sensor data using AWS services. The solution simulates **humidity sensor events** and processes them using a Spark streaming job on an EMR cluster. Processed data is stored in S3, and the system is monitored with CloudWatch alarms.

---

## ğŸš€ Overview

- **Data Simulation**: Python script generates humidity sensor data and uploads it to S3 every few minutes.  
- **Data Processing**: PySpark job runs on EMR, reads data from S3, cleans/normalizes it, and writes processed outputs back to S3.  
- **Storage**:  
  - Raw data â†’ S3 â€œbronzeâ€ bucket  
  - Transformed data â†’ S3 â€œgoldâ€ bucket (Parquet format)  
- **Monitoring**: CloudWatch alarms watch EMR job status and cluster health.  
- **Infrastructure as Code**: All AWS resources are provisioned with Terraform (no console steps).  

---

## ğŸ”§ Components

### ğŸ“ Data Ingestion
- `input_data_streaming.py` â†’ generates humidity events.  
- Data uploaded to `s3://iot-stream-raw-bronze/raw/`.  

### ğŸ’¡ Spark Transformation
- `spark_transformer.py` runs on EMR.  
- Cleans and validates data (filters bad records, normalizes fields).  
- Writes Parquet files to `s3://iot-stream-processed-gold/iot_transformed/`.  
- Uses checkpointing to avoid duplicates.  

### ğŸ§± Terraform Modules
- **s3.tf** â†’ Raw & Processed buckets  
- **emr.tf** â†’ EMR cluster + Spark step  
- **iam_emr.tf** â†’ IAM roles for EMR  
- **cloudwatch.tf** â†’ CloudWatch alarms for job/cluster health  
- **variables.tf** â†’ Parameterized values  
- **outputs.tf** â†’ EMR cluster ID  

---

## ğŸ“‚ Project Structure
â”œâ”€â”€ Scripts/
â”‚   â”œâ”€â”€ input_data_streaming.py     # Simulates humidity data â†’ S3
â”‚   â””â”€â”€ spark_transformer.py        # Spark job to clean & process data
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ s3.tf                       # S3 buckets
â”‚   â”œâ”€â”€ emr.tf                      # EMR cluster setup
â”‚   â”œâ”€â”€ iam_emr.tf                  # IAM roles
â”‚   â”œâ”€â”€ cloudwatch.tf               # CloudWatch alarms
â”‚   â”œâ”€â”€ variables.tf                # Variables
â”‚   â”œâ”€â”€ outputs.tf                  # Outputs
â”‚   â”œâ”€â”€ provider.tf                 # AWS provider config
â”‚   â””â”€â”€ main.tf                     # Terraform entry
â”œâ”€â”€ README.md
---

## ğŸš€ How to Run

### 1. Run Data Generator
```bash
cd Scripts
python input_data_streaming.py
#deploy
cd terraform
terraform init
terraform apply -auto-approve
#Check Outputs
	â€¢	Raw data in â†’ s3://iot-stream-raw-bronze/raw/
	â€¢	Processed data in â†’ s3://iot-stream-processed-gold/iot_transformed/
	â€¢	EMR â†’ Steps tab shows Spark job success/failure
	â€¢	CloudWatch â†’ check alarms & metrics
    #acceptance
    Requirement
Status
Use AWS SDK/Terraform for setup
âœ… Done
Python script simulates humidity data
âœ… Done
Spark job processes streaming data
âœ… Done
Transformed data stored in S3
âœ… Done
CloudWatch alarms configured
âœ… Done
All Infra via Terraform only
âœ… Done
Code hosted on GitHub
âœ… Done